# Lab 2: Transformer Encoder ğŸ—ï¸

> **Time:** 3-4 hours | **Difficulty:** Intermediate

---

## ğŸ“š Theory Brief

The Transformer Encoder processes input sequences using self-attention and feed-forward layers:

```
Input â†’ Embedding â†’ Positional Encoding
  â†“
[Multi-Head Attention â†’ Add & Norm â†’ FFN â†’ Add & Norm] Ã— N layers
  â†“
Output Embeddings
```

**Key Components:**
1. **Positional Encoding:** Add position information to embeddings
2. **Self-Attention:** Model relationships between tokens
3. **Feed-Forward:** Non-linear transformation
4. **Residual Connections:** Improve gradient flow
5. **Layer Normalization:** Stabilize training

---

## ğŸ¯ Learning Objectives

- Implement sinusoidal positional encoding
- Build Transformer encoder layers
- Stack multiple encoder layers
- Add classification head for downstream tasks

---

## ğŸ“ Exercises

### Exercise 1: Positional Encoding (30 mins)
Implement sinusoidal position embeddings.

### Exercise 2: Encoder Layer (60 mins)
Build single encoder layer with attention + FFN + residuals.

### Exercise 3: Full Encoder (45 mins)
Stack N encoder layers into complete encoder.

### Exercise 4: Text Classification (60 mins)
Add pooling and classification head.

---

## âœ… Solution

See `solution/01_transformer_encoder.py`

```bash
python solution/01_transformer_encoder.py
```

---

## ğŸ“ Key Takeaways

- Positional encoding adds position information (sinusoidal or learned)
- Residual connections enable deep networks
- Layer norm stabilizes training
- Encoder outputs contextual representations

---

## ğŸš€ Next: Lab 3 - Full Transformer!
