# Lab 4: Model Serving & Production Inference âš¡

> **Time:** 2-3 hours | **Difficulty:** Expert

## Topics
- TorchServe deployment
- Model quantization (INT8)
- TorchScript and ONNX export
- Dynamic batching
- Latency optimization

## Exercises
1. Export model to ONNX
2. Apply quantization
3. Deploy with TorchServe
4. Benchmark latency (<50ms p99)

## Solution
`solution/model_serving.py`

## Next: Lab 5 - Distillation!
