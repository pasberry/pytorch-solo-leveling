# Lab 5: Knowledge Distillation ðŸ“š

> **Time:** 2-3 hours | **Difficulty:** Expert

## Topics
- Teacher-student training
- Soft targets and temperature
- Feature distillation
- Model compression (10x+)
- Performance recovery

## Exercises
1. Train teacher model
2. Distill to student (10x smaller)
3. Compare performance
4. Achieve 95%+ of teacher accuracy

## Solution
`solution/knowledge_distillation.py`

## Next: Lab 6 - A/B Testing!
