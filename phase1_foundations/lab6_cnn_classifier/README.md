# Lab 6: CNN Image Classifier - Learning Visual Hierarchies üñºÔ∏è

> **Time:** 3-4 hours
> **Difficulty:** Intermediate
> **Goal:** Master Convolutional Neural Networks - the architecture that revolutionized computer vision

---

## üìñ Why This Lab Matters

In 2012, AlexNet won ImageNet with CNNs, crushing traditional methods by 10+ percentage points. This single moment sparked the deep learning revolution.

**Why CNNs Changed Everything:**
- Image classification: 28% error ‚Üí 3.5% error (superhuman!)
- Object detection: Enabled self-driving cars
- Face recognition: Unlocked smartphone security
- Medical imaging: Early cancer detection
- Content moderation: Keeping platforms safe

This lab teaches you the architecture behind:
- **Meta:** Face recognition, content understanding, AR/VR
- **Google:** Image search, Google Photos, YouTube content ID
- **Tesla:** Autopilot vision system
- **Healthcare:** Medical image analysis

**The Big Idea:** CNNs don't just classify pixels - they learn hierarchical visual features (edges ‚Üí textures ‚Üí parts ‚Üí objects).

---

## üß† The Big Picture: Why CNNs for Vision?

### The Problem: Naive Neural Networks Fail at Images

**Challenge: Images are HUGE**
```
Tiny image: 32√ó32 RGB = 3,072 pixels
ImageNet: 224√ó224 RGB = 150,528 pixels
HD image: 1920√ó1080 RGB = 6,220,800 pixels
```

**Naive approach: Fully-connected network**
```python
# Flatten image to vector
x = image.flatten()  # [150,528]

# Fully connected layer
hidden = Linear(150_528, 1024)  # 154 MILLION parameters!
```

**Three fatal problems:**

**1. Too Many Parameters**
```
Parameters = 150,528 √ó 1024 = 154M
Memory: 154M √ó 4 bytes = 617 MB (just one layer!)
Result: Overfits, can't generalize, slow to train
```

**2. No Spatial Structure**
```
# These two images are IDENTICAL to FC network:
Image 1: Cat in top-left corner
Image 2: Cat in bottom-right corner

Problem: FC network treats each pixel independently!
No understanding that nearby pixels form patterns
```

**3. No Translation Invariance**
```
FC network must learn "cat" separately for EVERY position!
- Cat at (0, 0): Learn features
- Cat at (0, 1): Learn same features again
- Cat at (1, 0): Learn same features again
- ...thousands of positions!
```

### The Solution: Convolutional Neural Networks

**Three key ideas that make CNNs work:**

**1. Local Connectivity (Receptive Fields)**
```
Instead of connecting to ALL pixels,
each neuron connects to a small LOCAL region

Fully Connected: 150k inputs ‚Üí 1k neurons (150M parameters)
Convolutional: 3√ó3 region ‚Üí 1 neuron ‚Üí slide across image
```

**2. Parameter Sharing (Same Filter Everywhere)**
```
Use the SAME learned filter across the entire image

Result: Detect "edge" at ANY position with SAME weights
Parameters: 3√ó3√ó3 = 27 (not 150 million!)
```

**3. Translation Invariance**
```
Filter slides across image ‚Üí Detects pattern anywhere
"Cat" detector works whether cat is left, right, center, anywhere!
```

### Real-World Analogy

**FC Network = Memorizing:**
```
Memorize: "If pixel 1=0.5, pixel 2=0.3, ..., pixel 150k=0.2 ‚Üí cat"
Problem: Must memorize every possible configuration
```

**CNN = Understanding Structure:**
```
Layer 1: Learn edge detectors (horizontal, vertical, diagonal)
Layer 2: Combine edges ‚Üí textures (fur, whiskers, eyes)
Layer 3: Combine textures ‚Üí parts (ears, nose, paws)
Layer 4: Combine parts ‚Üí objects (cat!)

Same features work ANYWHERE in the image!
```

---

## üî¨ Deep Dive: How Convolutions Work

### The Convolution Operation

**Convolution** is a mathematical operation that slides a small matrix (kernel/filter) over an image:

```
Input Image (5√ó5):
[1 2 3 0 1]
[0 1 2 3 1]
[1 0 1 2 0]
[2 1 0 1 3]
[1 0 2 1 0]

Filter/Kernel (3√ó3):
[1  0 -1]
[1  0 -1]
[1  0 -1]

How it works:
1. Place filter at top-left
2. Element-wise multiply
3. Sum all products ‚Üí one output value
4. Slide filter right by stride
5. Repeat across entire image
```

**Detailed Example:**

**Position (0,0):**
```
Input region:        Filter:          Multiply:
[1 2 3]             [1  0 -1]        [1  0 -3]
[0 1 2]      √ó      [1  0 -1]   =    [0  0 -2]
[1 0 1]             [1  0 -1]        [1  0 -1]

Sum: 1+0-3+0+0-2+1+0-1 = -4
Output[0,0] = -4
```

**Position (0,1):** (slide right)
```
Input region:        Filter:
[2 3 0]             [1  0 -1]
[1 2 3]      √ó      [1  0 -1]
[0 1 2]             [1  0 -1]

Sum: 2+0+0+1+0-3+0+0-2 = -2
Output[0,1] = -2
```

**Complete Output (3√ó3):**
```
[-4 -2  2]
[-3 -1  3]
[-2  0  2]
```

### What Did This Filter Detect?

This vertical edge detector responds to:
- **High values (white):** Bright-to-dark transition (left to right)
- **Negative values:** Dark-to-bright transition
- **Zero:** Uniform region (no edge)

### Mathematical Definition

**2D Convolution:**
```
(I * K)[i,j] = Œ£Œ£ I[i+m, j+n] √ó K[m,n]
               m n

Where:
- I: Input image/feature map
- K: Kernel/filter
- i,j: Output position
- m,n: Kernel indices
- *: Convolution operator (not multiplication!)
```

**PyTorch Implementation:**
```python
import torch.nn as nn

# Single convolutional layer
conv = nn.Conv2d(
    in_channels=3,      # RGB input
    out_channels=64,    # 64 different filters
    kernel_size=3,      # 3√ó3 filters
    stride=1,           # Slide 1 pixel at a time
    padding=1           # Add 1-pixel border (preserve size)
)

# Input: (batch, channels, height, width)
x = torch.randn(32, 3, 224, 224)  # 32 RGB images, 224√ó224
output = conv(x)  # (32, 64, 224, 224) - 64 feature maps
```

### Key Convolution Parameters

**1. Kernel Size**
```
3√ó3: Standard choice (small receptive field)
5√ó5: Larger receptive field (more context)
1√ó1: "Network in network" (change channels, no spatial mixing)
7√ó7: Large initial layer (AlexNet, ResNet stem)
```

**2. Stride**
```
stride=1: Output size ‚âà input size (dense features)
stride=2: Output size = input/2 (downsampling)
```

**3. Padding**
```
padding=0: Output shrinks (valid convolution)
padding=1 (for 3√ó3): Preserve size (same convolution)
padding=k//2: General rule for preserving size
```

**4. Dilation** (Advanced)
```
dilation=1: Standard convolution
dilation=2: Skip every other pixel (larger receptive field)
```

**Output Size Formula:**
```
output_size = (input_size - kernel_size + 2√ópadding) / stride + 1

Example: 224√ó224 input, 3√ó3 kernel, stride=1, padding=1
output_size = (224 - 3 + 2√ó1) / 1 + 1 = 224 ‚úì (same size)

Example: 224√ó224 input, 3√ó3 kernel, stride=2, padding=1
output_size = (224 - 3 + 2√ó1) / 2 + 1 = 112 (downsampled)
```

---

## üìä Receptive Fields & Feature Hierarchies

### Receptive Field: What Each Neuron "Sees"

**Receptive field** = region of input image that affects a neuron's output

**Layer 1 (3√ó3 conv):**
```
Each neuron sees 3√ó3 pixels
Learns: Edges, colors, simple patterns
```

**Layer 2 (stack another 3√ó3 conv):**
```
Each neuron sees 5√ó5 pixels (in original image)
How: 3√ó3 region of Layer 1 √ó each Layer 1 neuron sees 3√ó3
Learns: Textures, corners, simple shapes
```

**Layer 3:**
```
Receptive field: 7√ó7 pixels
Learns: Parts (eyes, ears, wheels)
```

**Layer 4+:**
```
Receptive field: 15√ó15+ pixels
Learns: Full objects
```

**Key Insight:** Deep networks see more context!

### The Feature Hierarchy

CNNs learn increasingly abstract representations:

```
Input: Raw pixels (224√ó224√ó3)
   ‚Üì
Layer 1: Edge Detectors (3√ó3 conv)
- Horizontal edges
- Vertical edges
- Diagonal edges
- Color boundaries
[224√ó224√ó64 feature maps]
   ‚Üì
Layer 2: Texture Detectors (3√ó3 conv)
- Fur patterns
- Fabric weaves
- Brick patterns
- Wood grain
[112√ó112√ó128 feature maps]
   ‚Üì
Layer 3: Part Detectors (3√ó3 conv)
- Eyes
- Wheels
- Windows
- Ears
[56√ó56√ó256 feature maps]
   ‚Üì
Layer 4: Object Detectors (3√ó3 conv)
- Faces
- Cars
- Buildings
- Animals
[28√ó28√ó512 feature maps]
   ‚Üì
Global Average Pooling
[512 features]
   ‚Üì
Fully Connected ‚Üí Classes
[1000 outputs: cat, dog, car, ...]
```

**Visualization Insight:**
Researchers have visualized what filters detect:
- Early layers: Gabor filters, color blobs (like V1 in human visual cortex!)
- Middle layers: Textures, patterns
- Late layers: Object parts, full objects

This hierarchy emerges **automatically** from training - not hand-designed!

---

## üéØ Pooling Layers: Downsampling

### Why Pooling?

**Two problems to solve:**
1. **Too many features:** 224√ó224√ó64 = 3.2M values per image!
2. **Need invariance:** Small translations shouldn't change output

**Solution:** Downsample feature maps

### Max Pooling

**Most common:** Take maximum value in each region

```
Input (4√ó4):              2√ó2 Max Pool:
[1 3 2 4]                 [3 4]
[2 1 5 3]        ‚Üí        [8 9]
[7 8 1 2]
[3 6 9 4]

How:
Top-left 2√ó2: max(1,3,2,1) = 3
Top-right 2√ó2: max(2,4,5,3) = 4
Bottom-left 2√ó2: max(7,8,3,6) = 8
Bottom-right 2√ó2: max(1,2,9,4) = 9
```

**Properties:**
- Reduces spatial dimensions (224√ó224 ‚Üí 112√ó112)
- Keeps strongest activations (most confident features)
- Provides small translation invariance
- No learnable parameters

### Average Pooling

**Alternative:** Take average of each region

```
Input (4√ó4):              2√ó2 Average Pool:
[1 3 2 4]                 [1.75 3.5]
[2 1 5 3]        ‚Üí        [6.0  4.0]
[7 8 1 2]
[3 6 9 4]

Top-left: (1+3+2+1)/4 = 1.75
```

**Properties:**
- Smoother downsampling
- Preserves more information
- Used in later architectures (Global Average Pooling)

### Global Average Pooling (GAP)

**Modern replacement for FC layers:**

```
Input: (7√ó7√ó512) feature maps
GAP: Average each feature map ‚Üí (512,) vector

For each of 512 channels:
    output[i] = mean(feature_map[i])

Result: 512 features (one per filter)
```

**Why GAP is better than FC:**
- **Fewer parameters:** No weights to learn!
- **Works with any input size:** Can process different image sizes
- **Less overfitting:** Regularization by design
- **Interpretable:** Each output corresponds to a filter

---

## üèóÔ∏è Modern CNN Architectures

### Classic CNNs (Historical Context)

**LeNet-5 (1998):**
```
[Conv-Pool-Conv-Pool-FC-FC]
First successful CNN (handwritten digits)
Parameters: ~60K
```

**AlexNet (2012):**
```
ImageNet breakthrough (16.4% error)
[Conv-Pool-Conv-Pool-Conv-Conv-Conv-Pool-FC-FC-FC]
Parameters: 60M
Innovations: ReLU, Dropout, GPU training
```

**VGG-16 (2014):**
```
Simple, deep architecture (13.6% error)
[Conv-Conv-Pool] √ó 5 ‚Üí [FC-FC-FC]
Parameters: 138M
Key insight: Stack small (3√ó3) filters instead of large ones
```

### ResNet: The Skip Connection Revolution (2015)

**The Problem: Vanishing Gradients in Deep Networks**

```
50-layer network should ‚â• 20-layer network (can just copy)
But in practice: Deeper networks performed WORSE!
Why? Gradients vanish during backprop through many layers
```

**The Solution: Residual Connections (Skip Connections)**

**Standard block:**
```
x ‚Üí Conv ‚Üí ReLU ‚Üí Conv ‚Üí ReLU ‚Üí y
Problem: Gradients must flow through all transformations
```

**Residual block:**
```
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ                 ‚Üì
x ‚Üí Conv ‚Üí ReLU ‚Üí Conv ‚Üí ADD ‚Üí ReLU ‚Üí y
                        ‚Üë
                     identity

y = F(x) + x

Where F(x) = learned transformation
```

**Why it works:**

**1. Gradient Flow:**
```
Backprop through residual block:
‚àÇLoss/‚àÇx = ‚àÇLoss/‚àÇy √ó (‚àÇF/‚àÇx + 1)
                              ‚Üë
                        Always ‚â• 1!

Result: Gradients flow directly through skip connections
No vanishing gradients even in 100+ layer networks!
```

**2. Easy Optimization:**
```
Network can learn identity function easily:
If F(x) = 0, then y = x (perfect identity)

With residual: Learn small refinements to identity
Without residual: Learn entire transformation from scratch
```

**3. Feature Reuse:**
```
Early layer features can skip to late layers
Enables gradient highway through the network
```

**ResNet Architectures:**

**ResNet-18/34:** Basic blocks
```python
class BasicBlock(nn.Module):
    def forward(self, x):
        identity = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out += identity  # Skip connection!
        out = self.relu(out)
        return out
```

**ResNet-50/101/152:** Bottleneck blocks (1√ó1 ‚Üí 3√ó3 ‚Üí 1√ó1)
```python
class Bottleneck(nn.Module):
    def forward(self, x):
        identity = x
        out = self.conv1(x)  # 1√ó1: reduce channels
        out = self.conv2(out)  # 3√ó3: spatial mixing
        out = self.conv3(out)  # 1√ó1: expand channels
        out += identity
        return self.relu(out)
```

**Impact:**
- ResNet-152: 3.57% error on ImageNet (superhuman!)
- Enabled training of 1000+ layer networks
- Foundation for modern vision: Mask R-CNN, Feature Pyramid Networks
- Inspired Transformers (attention = learned skip connections)

---

## üî¨ Batch Normalization in CNNs

### The Problem: Internal Covariate Shift

**During training, layer inputs change distribution:**
```
Epoch 1: Layer 3 input ~ N(0, 1)
Epoch 10: Layer 3 input ~ N(5, 10)  # Distribution shifted!

Problem: Layer 3 must constantly readapt
Result: Slow training, hard to optimize
```

### The Solution: Batch Normalization

**Normalize each layer's inputs:**

```
For each channel in the batch:
1. Compute mean (Œº) and variance (œÉ¬≤) across batch
2. Normalize: x_norm = (x - Œº) / ‚àö(œÉ¬≤ + Œµ)
3. Scale and shift: y = Œ≥ √ó x_norm + Œ≤

Where Œ≥, Œ≤ are learnable parameters
```

**Example:**
```python
# Input: (batch=32, channels=64, height=28, width=28)
bn = nn.BatchNorm2d(64)

# For each of 64 channels:
#   Compute Œº, œÉ¬≤ across all 32√ó28√ó28 = 25,088 values
#   Normalize all values in that channel
#   Apply learned Œ≥, Œ≤ for that channel
```

**Why it works:**

**1. Stable Distributions:**
```
Every layer sees normalized inputs (mean=0, std=1)
Consistent training signal throughout network
```

**2. Higher Learning Rates:**
```
Normalization smooths loss landscape
Can use 10-100x larger learning rates
Faster convergence
```

**3. Regularization:**
```
Normalization depends on batch statistics
Adds noise (different batches ‚Üí different normalization)
Acts like dropout (reduces overfitting)
```

**4. Reduces Gradient Dependence:**
```
Gradients don't depend as much on parameter scale
More stable training
```

**BatchNorm Placement:**
```python
# Standard placement:
Conv ‚Üí BatchNorm ‚Üí ReLU

# Why this order:
# Conv: Linear transformation
# BN: Normalize (stabilize)
# ReLU: Nonlinearity
```

**Training vs Inference:**
```
Training: Use batch statistics (Œº, œÉ from current batch)
Inference: Use running statistics (Œº, œÉ averaged over training)

Why: Single image at test time ‚Üí no "batch" to compute statistics from
```

**Impact:**
- Enabled training of very deep networks
- Standard component in modern CNNs
- Inspired Layer Norm (Transformers), Group Norm, Instance Norm

---

## üéØ Learning Objectives

By the end of this lab, you'll understand:

**Theoretical Understanding:**
- Why CNNs work for images (locality, translation invariance)
- How convolution operation builds feature hierarchies
- Receptive fields and how they grow with depth
- Why pooling provides invariance and reduces computation
- How skip connections enable deep networks
- How batch normalization stabilizes training

**Practical Skills:**
- Build CNN architectures from scratch
- Implement residual blocks
- Use batch normalization effectively
- Design CNN architectures for different problems
- Train CNNs on image classification
- Achieve >85% on CIFAR-10
- Apply data augmentation
- Debug common CNN issues

---

## üîë Key Concepts

### 1. CNN Design Principles

**Rule 1: Stack small filters**
```
2√ó (3√ó3 conv) > 1√ó (5√ó5 conv)
- Same receptive field (5√ó5)
- Fewer parameters: 2√ó9 = 18 vs 25
- More nonlinearity (2 ReLUs vs 1)
```

**Rule 2: Downsample gradually**
```
Good: 224 ‚Üí 112 ‚Üí 56 ‚Üí 28 ‚Üí 14 ‚Üí 7
Bad:  224 ‚Üí 7 (too aggressive, loses information)
```

**Rule 3: Increase channels as you downsample**
```
64 channels @ 224√ó224
128 channels @ 112√ó112
256 channels @ 56√ó56
512 channels @ 28√ó28

Why: Trade spatial resolution for feature diversity
```

**Rule 4: Use skip connections for depth >20 layers**
```
Without: Vanishing gradients
With: Can train 100+ layers
```

### 2. Data Augmentation for CNNs

**Training CNNs requires lots of data. Augmentation creates more:**

```python
from torchvision import transforms

transform = transforms.Compose([
    transforms.RandomCrop(32, padding=4),      # Random translation
    transforms.RandomHorizontalFlip(),         # Flip 50% of images
    transforms.ColorJitter(                    # Random color changes
        brightness=0.2,
        contrast=0.2,
        saturation=0.2
    ),
    transforms.RandomRotation(15),             # Rotate ¬±15 degrees
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5])
])
```

**Why it works:**
- Prevents overfitting (memorizing training set)
- Makes model invariant to transformations
- Effectively multiplies dataset size
- Standard practice: improves accuracy 5-10%

### 3. Transfer Learning

**Don't train from scratch - use pre-trained weights!**

```python
import torchvision.models as models

# Load ResNet pre-trained on ImageNet
resnet = models.resnet50(pretrained=True)

# Freeze early layers (feature extractor)
for param in resnet.parameters():
    param.requires_grad = False

# Replace final layer for your task
resnet.fc = nn.Linear(2048, num_classes)

# Only train final layer (fast, less data needed)
```

**When to use:**
- Limited data (<10k images)
- Similar domain (natural images)
- Want faster training

**Results:**
- With transfer: 90% accuracy with 1k images
- Without transfer: 90% accuracy needs 100k+ images

---

## üß™ Exercises

### Exercise 1: Build a Simple CNN
**File:** `01_simple_cnn.py`

**What You'll Learn:**
- Stack Conv ‚Üí ReLU ‚Üí Pool layers
- Calculate output shapes
- Add fully connected classifier head
- Train on CIFAR-10

**Why It Matters:**
Understanding basic CNN building blocks is essential before using complex architectures.

**Architecture:**
```python
# TODO: Implement this architecture
Input: 32√ó32√ó3 (CIFAR-10)
Conv(3‚Üí32, 3√ó3) ‚Üí ReLU ‚Üí MaxPool(2√ó2)  # 16√ó16√ó32
Conv(32‚Üí64, 3√ó3) ‚Üí ReLU ‚Üí MaxPool(2√ó2)  # 8√ó8√ó64
Conv(64‚Üí128, 3√ó3) ‚Üí ReLU ‚Üí MaxPool(2√ó2) # 4√ó4√ó128
Flatten ‚Üí FC(2048‚Üí512) ‚Üí ReLU ‚Üí Dropout(0.5)
FC(512‚Üí10) ‚Üí Softmax

# TODO: Calculate parameters
# TODO: Train for 20 epochs
# TODO: Achieve >70% test accuracy
```

**Expected Results:**
- Parameters: ~1.2M
- Train accuracy: ~90%
- Test accuracy: ~72%

---

### Exercise 2: Add Batch Normalization
**File:** `02_batchnorm_cnn.py`

**What You'll Learn:**
- Add BatchNorm after each Conv layer
- Compare convergence: with vs without BN
- Observe training stability
- Use higher learning rates

**Why It Matters:**
BatchNorm is **standard practice** - every modern CNN uses it.

**Tasks:**
```python
# TODO: Modify Exercise 1 architecture
Conv ‚Üí BatchNorm ‚Üí ReLU ‚Üí Pool

# TODO: Train with same hyperparameters
# TODO: Train with 10x higher learning rate
# TODO: Compare convergence speed
# TODO: Plot loss curves: BN vs no-BN
```

**Expected Results:**
- Faster convergence (5 epochs vs 20)
- Higher learning rate works (0.01 vs 0.001)
- Test accuracy: ~75% (3% improvement!)

---

### Exercise 3: Implement ResNet Block
**File:** `03_resnet_block.py`

**What You'll Learn:**
- Implement residual connections
- Handle dimension mismatches (1√ó1 projection)
- Stack residual blocks
- Train deeper networks (20+ layers)

**Why It Matters:**
ResNets are the foundation of modern computer vision. Understanding skip connections is crucial.

**Tasks:**
```python
# TODO: Implement BasicBlock with skip connection
class BasicBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        # TODO: Conv ‚Üí BN ‚Üí ReLU ‚Üí Conv ‚Üí BN
        # TODO: Add shortcut (1√ó1 conv if dimensions change)
        pass

    def forward(self, x):
        # TODO: identity = x
        # TODO: out = F(x)  # Main path
        # TODO: out += identity  # Skip connection
        # TODO: return ReLU(out)
        pass

# TODO: Build ResNet-18 style network
# TODO: Train on CIFAR-10
# TODO: Achieve >80% accuracy
```

**Expected Results:**
- Can train 20+ layer network without degradation
- Test accuracy: ~82%
- Gradients flow smoothly (no vanishing)

---

### Exercise 4: Data Augmentation & Regularization
**File:** `04_augmentation.py`

**What You'll Learn:**
- Apply random transformations during training
- Use augmentation to prevent overfitting
- Combine with dropout and weight decay
- Reach competition-level accuracy

**Why It Matters:**
Top CIFAR-10 models use heavy augmentation - essential for state-of-the-art results.

**Tasks:**
```python
# TODO: Implement aggressive augmentation
transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(0.2, 0.2, 0.2),
    transforms.RandomRotation(15),
    transforms.ToTensor(),
    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))
])

# TODO: Add dropout (0.5) in classifier
# TODO: Add weight decay (1e-4) in optimizer
# TODO: Train for 100 epochs with LR schedule
# TODO: Achieve >85% accuracy
```

**Expected Results:**
- Train accuracy: ~95%
- Test accuracy: >85%
- Reduces overfitting by ~10%

---

### Exercise 5: Transfer Learning with Pre-trained ResNet
**File:** `05_transfer_learning.py`

**What You'll Learn:**
- Load pre-trained ImageNet weights
- Freeze feature extractor
- Fine-tune final layers
- Train with limited data

**Why It Matters:**
In production, you rarely train from scratch. Transfer learning is the standard approach.

**Tasks:**
```python
# TODO: Load pretrained ResNet-18
model = torchvision.models.resnet18(pretrained=True)

# TODO: Freeze all layers except final FC
for param in model.parameters():
    param.requires_grad = False
model.fc = nn.Linear(512, 10)  # CIFAR-10

# TODO: Train only final layer (fast!)
# TODO: Compare: scratch vs transfer learning
# TODO: Reduce training data to 10% - observe transfer learning advantage
```

**Expected Results:**
- Full data: 87% accuracy in 10 epochs (vs 85% in 100 from scratch)
- 10% data: 75% accuracy (vs 45% from scratch)
- 10x faster training

---

## üìù Starter Code

See `starter/` directory for templates with detailed TODOs and comments.

---

## ‚úÖ Solutions

Full implementations in `solution/` directory.

**Debugging Tips:**
1. Print shapes after each layer
2. Visualize filters and activations
3. Check for exploding/vanishing gradients
4. Start simple, add complexity gradually

---

## üéì Key Takeaways

After completing this lab, you should understand:

1. **CNNs exploit spatial structure** - Local connectivity + parameter sharing
2. **Convolution learns feature hierarchies** - Edges ‚Üí textures ‚Üí parts ‚Üí objects
3. **Pooling provides invariance** - Small translations don't change output
4. **Skip connections enable depth** - Gradients flow through network
5. **BatchNorm stabilizes training** - Normalize layer inputs
6. **Augmentation prevents overfitting** - Artificially expand dataset
7. **Transfer learning leverages pre-training** - Don't start from scratch

**The Fundamental Insight:**
```
CNNs don't memorize pixels - they learn reusable visual concepts.
The same edge detector works everywhere in the image.
This is why CNNs generalize so well.
```

---

## üîó Connections to Production ML

### Why This Matters at Scale

**Meta's Vision Systems:**
- Feed ranking: Classify billions of images daily
- Content moderation: Detect harmful content
- Face recognition: Tag photos automatically
- AR/VR: Real-time scene understanding
- Architecture: ResNet variants, optimized for mobile

**Self-Driving Cars:**
- Tesla: 8 cameras ‚Üí CNN feature extraction ‚Üí planning
- Process: 36 frames/sec, 2048√ó1536 resolution per camera
- Requirement: Real-time inference (<50ms)
- Solution: Efficient CNN architectures (MobileNet, EfficientNet)

**Medical Imaging:**
- Chest X-ray analysis: Detect pneumonia, COVID-19
- Skin cancer detection: Dermatologist-level accuracy
- Retinal imaging: Early diabetes detection
- Architecture: ResNet-50, DenseNet (pre-trained + fine-tuned)

**Content Understanding:**
- Google Photos: Automatic organization (people, places, things)
- Pinterest: Visual search (similar items)
- Amazon: Product recognition from photos
- Scale: Billions of images, millisecond latency

### Real-World Considerations

**1. Latency Requirements:**
```
Batch processing (training): Throughput matters
Real-time (self-driving): Latency critical (<50ms)
Mobile (AR): Power efficiency matters

Solution: Architecture search, pruning, quantization
```

**2. Data Distribution Shift:**
```
Training: ImageNet (natural images)
Production: User photos (filters, crops, low quality)

Solution: Data augmentation, domain adaptation, continual learning
```

**3. Class Imbalance:**
```
Cat vs dog: 50/50 (balanced)
Fraud detection: 99.9% normal, 0.1% fraud (imbalanced)

Solution: Weighted loss, oversampling, focal loss
```

---

## üöÄ Next Steps

Once you've completed all exercises:

1. **Visualize learned features** - Use hooks to extract activations
2. **Experiment with architectures** - Try different depths, widths
3. **Kaggle competition** - Apply skills to real challenge
4. **Complete Phase 1 Checkpoint Project** - Put it all together!

---

## üí™ Bonus Challenges

1. **Visualize Filters and Activations**
   - Extract and plot learned Conv filters
   - Visualize feature maps for test images
   - Understand what network has learned

2. **Implement Different Architectures**
   - VGG-11 (simple, deep)
   - DenseNet (dense connections)
   - MobileNet (efficient for mobile)
   - Compare parameters, accuracy, speed

3. **Grad-CAM Visualization**
   - Implement class activation mapping
   - See where network looks to make decisions
   - Debug misclassifications

4. **Advanced Augmentation**
   - Cutout (random patches masked)
   - Mixup (blend two images)
   - AutoAugment (learned policies)
   - Observe impact on accuracy

5. **Quantization**
   - Convert FP32 model to INT8
   - Measure speedup and accuracy loss
   - Deploy efficient model

6. **Neural Architecture Search**
   - Implement simple NAS
   - Search over kernel sizes, depths
   - Find optimal architecture automatically

---

## üìö Additional Resources

**Essential Reading:**
- [CS231n: Convolutional Neural Networks](http://cs231n.stanford.edu/) - Best CNN course
- [Deep Residual Learning (ResNet paper)](https://arxiv.org/abs/1512.03385)
- [Batch Normalization paper](https://arxiv.org/abs/1502.03167)

**Papers (Landmark Architectures):**
- [AlexNet (2012)](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) - Started deep learning revolution
- [VGG (2014)](https://arxiv.org/abs/1409.1556) - Power of depth
- [ResNet (2015)](https://arxiv.org/abs/1512.03385) - Skip connections
- [EfficientNet (2019)](https://arxiv.org/abs/1905.11946) - Scaling CNNs

**Interactive:**
- [CNN Explainer](https://poloclub.github.io/cnn-explainer/) - Visualize CNN operations
- [Distill.pub: Feature Visualization](https://distill.pub/2017/feature-visualization/) - What do CNNs learn?
- [TensorFlow Playground](https://playground.tensorflow.org/) - Neural network intuition

**Tools:**
- [torchvision.models](https://pytorch.org/vision/stable/models.html) - Pre-trained models
- [timm](https://github.com/rwightman/pytorch-image-models) - 500+ CNN architectures
- [Grad-CAM](https://github.com/jacobgil/pytorch-grad-cam) - Visualization

---

## ü§î Common Pitfalls & Solutions

### Pitfall 1: Wrong Input Shape
```python
# ‚ùå CIFAR-10 is 32√ó32, not 28√ó28!
model = CNN()  # Expects 28√ó28
x = cifar_batch  # 32√ó32
output = model(x)  # Shape mismatch!

# ‚úì Verify shapes
print(f"Input: {x.shape}")  # [32, 3, 32, 32]
print(f"After conv1: {...}")  # Track shapes through network
```

### Pitfall 2: Forgetting to Normalize
```python
# ‚ùå Raw pixels [0, 255] ‚Üí unstable training
transform = transforms.ToTensor()  # Only converts to [0, 1]

# ‚úì Normalize to mean=0, std=1
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))
])
```

### Pitfall 3: BatchNorm in Wrong Order
```python
# ‚ùå Wrong: ReLU before BN
Conv ‚Üí ReLU ‚Üí BatchNorm  # BN after nonlinearity (less effective)

# ‚úì Correct: BN before ReLU
Conv ‚Üí BatchNorm ‚Üí ReLU  # Standard order
```

### Pitfall 4: Data Augmentation at Test Time
```python
# ‚ùå Augmentation during evaluation!
train_transform = test_transform = transforms.Compose([
    transforms.RandomCrop(32, padding=4),  # Random!
    transforms.RandomHorizontalFlip(),
    ...
])

# ‚úì Different transforms for train/test
train_transform = transforms.Compose([...augmentation...])
test_transform = transforms.Compose([...no augmentation...])
```

### Pitfall 5: Dimension Mismatch in Skip Connections
```python
# ‚ùå Can't add different shapes!
x: (32, 64, 28, 28)
out: (32, 128, 14, 14)
out += x  # Error!

# ‚úì Use 1√ó1 conv projection when dimensions change
if x.shape != out.shape:
    x = self.shortcut(x)  # 1√ó1 conv to match dimensions
out += x
```

---

## üí° Pro Tips

1. **Always verify shapes** - Print after each layer during debugging
2. **Use BatchNorm** - Faster training, higher accuracy, free regularization
3. **Augment aggressively** - 5-10% accuracy boost for free
4. **Transfer learning for <10k images** - Don't train from scratch
5. **Start with proven architectures** - ResNet-18 is a great baseline
6. **Monitor both train and test** - Detect overfitting early
7. **LR schedule matters** - Reduce LR when plateau
8. **Visualize predictions** - See what model gets wrong

**Golden Rule:**
```
Simple architecture + good data augmentation + transfer learning
>
Complex custom architecture trained from scratch
```

---

## ‚ú® You're Ready When...

- [ ] You can explain why CNNs work for images
- [ ] You understand convolution operation mathematically
- [ ] You can calculate receptive fields
- [ ] You know when and why to use pooling
- [ ] You've implemented residual blocks with skip connections
- [ ] You understand how BatchNorm stabilizes training
- [ ] You can apply data augmentation effectively
- [ ] You've achieved >85% on CIFAR-10
- [ ] You can use transfer learning
- [ ] You understand the feature hierarchy CNNs learn

**Critical Understanding:**
- CNNs learn reusable features, not pixel patterns
- Skip connections are essential for deep networks
- Augmentation and normalization are not optional
- Transfer learning is the production default

**Congratulations!** You've completed Phase 1. You now understand:
- Tensors & Autograd
- Neural Network Modules
- Training Loops
- Data Loading
- GPU Acceleration
- Convolutional Networks

**Next:** Phase 1 Checkpoint Project - Build an end-to-end image classifier!

